"""
Integra√ß√£o Ollama/Llama - Processamento inteligente de respostas
Melhora as respostas do agente com IA local
"""
import json
import requests
from typing import Dict, List, Any, Optional
from datetime import datetime

class OllamaIntegration:
    """Integra√ß√£o com Ollama para processamento de linguagem natural"""
    
    def __init__(self, base_url: str = "http://localhost:11434", model: str = "llama3.2:3b"):
        """Inicializa integra√ß√£o Ollama"""
        
        self.base_url = base_url
        self.model = model
        self.available = self._test_connection()
        
        if self.available:
            print(f"[OLLAMA] Conectado - Modelo: {self.model}")
        else:
            print("[OLLAMA] N√£o dispon√≠vel - funcionando sem IA")
    
    def _test_connection(self) -> bool:
        """Testa conex√£o com Ollama"""
        
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def enhance_response(self, user_query: str, data_analysis: str, leads_data: List[Dict]) -> str:
        """Melhora resposta usando Llama"""
        
        if not self.available:
            return data_analysis  # Retorna an√°lise original se Ollama indispon√≠vel
        
        # Prepara contexto para Llama
        context = self._prepare_context(user_query, data_analysis, leads_data)
        
        # Prompt otimizado para an√°lise de dados
        prompt = f"""Voc√™ √© um especialista em an√°lise de dados de marketing imobili√°rio. 
        
Consulta do usu√°rio: "{user_query}"

Dados analisados:
{data_analysis}

Contexto adicional dos dados:
{context}

Sua tarefa:
1. Forne√ßa uma resposta clara e objetiva
2. Destaque insights importantes
3. Use linguagem profissional mas acess√≠vel
4. Inclua n√∫meros espec√≠ficos quando relevante
5. Sugira pr√≥ximos passos se apropriado

Resposta:"""

        try:
            response = requests.post(
                f"{self.base_url}/api/generate",
                json={
                    "model": self.model,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.3,  # Mais conservador para dados
                        "top_p": 0.9,
                        "max_tokens": 512
                    }
                },
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                enhanced_text = result.get("response", "").strip()
                
                if enhanced_text and len(enhanced_text) > 50:
                    return f"{enhanced_text}\n\n---\nüìä Fonte: API CVDW Real | Processado com IA Local"
                else:
                    return data_analysis  # Fallback se resposta muito curta
            else:
                return data_analysis  # Fallback se erro HTTP
                
        except Exception as e:
            print(f"[OLLAMA] Erro: {str(e)}")
            return data_analysis  # Fallback se erro
    
    def _prepare_context(self, query: str, analysis: str, leads_data: List[Dict]) -> str:
        """Prepara contexto adicional para Llama"""
        
        context_parts = []
        
        # Estat√≠sticas b√°sicas
        if leads_data:
            total_leads = len(leads_data)
            context_parts.append(f"Total de leads analisados: {total_leads}")
            
            # Campos √∫nicos presentes
            if leads_data:
                sample_lead = leads_data[0]
                context_parts.append(f"Dados dispon√≠veis por lead: {len(sample_lead)} campos")
                
                # Situa√ß√µes √∫nicas
                situacoes = set(lead.get('situacao', '') for lead in leads_data[:100])  # Sample
                if situacoes:
                    context_parts.append(f"Situa√ß√µes encontradas: {', '.join(list(situacoes)[:5])}")
                
                # Origens √∫nicas
                origens = set(lead.get('origem_nome', '') for lead in leads_data[:100])
                if origens:
                    context_parts.append(f"Principais origens: {', '.join(list(origens)[:5])}")
        
        return "\n".join(context_parts)
    
    def generate_insights(self, leads_data: List[Dict], analysis_type: str = "general") -> List[str]:
        """Gera insights inteligentes usando Llama"""
        
        if not self.available or not leads_data:
            return []
        
        # Amostra dos dados para an√°lise
        sample_size = min(50, len(leads_data))
        sample_leads = leads_data[:sample_size]
        
        # Estat√≠sticas r√°pidas
        total_leads = len(leads_data)
        situacoes = {}
        origens = {}
        
        for lead in sample_leads:
            situacao = lead.get('situacao', 'N√£o informado')
            origem = lead.get('origem_nome', 'N√£o informado')
            
            situacoes[situacao] = situacoes.get(situacao, 0) + 1
            origens[origem] = origens.get(origem, 0) + 1
        
        # Prepara prompt para insights
        prompt = f"""Analise estes dados de leads imobili√°rios e gere insights valiosos:

Total de leads: {total_leads}
Amostra analisada: {sample_size} leads

Situa√ß√µes encontradas:
{json.dumps(situacoes, indent=2)}

Origens dos leads:
{json.dumps(origens, indent=2)}

Gere 3-5 insights pr√°ticos e acion√°veis baseados nestes dados. Foque em:
- Padr√µes interessantes
- Oportunidades de melhoria
- Recomenda√ß√µes estrat√©gicas

Formato: Lista de insights curtos e objetivos.

Insights:"""

        try:
            response = requests.post(
                f"{self.base_url}/api/generate",
                json={
                    "model": self.model,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.4,
                        "top_p": 0.9,
                        "max_tokens": 300
                    }
                },
                timeout=25
            )
            
            if response.status_code == 200:
                result = response.json()
                insights_text = result.get("response", "").strip()
                
                # Processa resposta em lista
                insights = []
                for line in insights_text.split('\n'):
                    line = line.strip()
                    if line and (line.startswith('-') or line.startswith('‚Ä¢') or line.startswith('*')):
                        insight = line.lstrip('-‚Ä¢* ').strip()
                        if len(insight) > 10:  # Filtra linhas muito curtas
                            insights.append(insight)
                
                return insights[:5]  # M√°ximo 5 insights
                
        except Exception as e:
            print(f"[OLLAMA] Erro ao gerar insights: {str(e)}")
            
        return []
    
    def classify_query_intent(self, query: str) -> Dict[str, Any]:
        """Classifica inten√ß√£o da consulta usando Llama"""
        
        if not self.available:
            return self._basic_classification(query)
        
        prompt = f"""Analise esta consulta sobre dados de marketing imobili√°rio e classifique:

Consulta: "{query}"

Classifique em uma das categorias:
- QUANTITATIVO: Perguntas sobre n√∫meros, totais, contagens
- PERFORMANCE: An√°lise de desempenho, rankings, compara√ß√µes
- TEMPORAL: Consultas sobre per√≠odos, datas, tend√™ncias
- ORIGEM: An√°lise de canais, fontes, campanhas
- SITUACAO: Status de leads, funil de vendas
- RESPONSAVEL: Performance de equipe, SDRs, corretores
- GERAL: Consultas gerais ou explora√ß√µes

Responda APENAS a categoria, sem explica√ß√µes.

Categoria:"""

        try:
            response = requests.post(
                f"{self.base_url}/api/generate",
                json={
                    "model": self.model,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.1,  # Muito conservador para classifica√ß√£o
                        "max_tokens": 20
                    }
                },
                timeout=10
            )
            
            if response.status_code == 200:
                result = response.json()
                classification = result.get("response", "").strip().upper()
                
                valid_categories = ["QUANTITATIVO", "PERFORMANCE", "TEMPORAL", "ORIGEM", "SITUACAO", "RESPONSAVEL", "GERAL"]
                
                if classification in valid_categories:
                    return {
                        "category": classification,
                        "confidence": 0.8,
                        "source": "llama"
                    }
                    
        except Exception as e:
            print(f"[OLLAMA] Erro na classifica√ß√£o: {str(e)}")
        
        # Fallback para classifica√ß√£o b√°sica
        return self._basic_classification(query)
    
    def _basic_classification(self, query: str) -> Dict[str, Any]:
        """Classifica√ß√£o b√°sica sem IA"""
        
        query_lower = query.lower()
        
        if any(word in query_lower for word in ["quantos", "total", "numero"]):
            return {"category": "QUANTITATIVO", "confidence": 0.7, "source": "basic"}
        elif any(word in query_lower for word in ["sdr", "responsavel", "corretor"]):
            return {"category": "RESPONSAVEL", "confidence": 0.7, "source": "basic"}
        elif any(word in query_lower for word in ["origem", "canal", "campanha"]):
            return {"category": "ORIGEM", "confidence": 0.7, "source": "basic"}
        elif any(word in query_lower for word in ["situacao", "status", "performance"]):
            return {"category": "PERFORMANCE", "confidence": 0.7, "source": "basic"}
        else:
            return {"category": "GERAL", "confidence": 0.5, "source": "basic"}
    
    def get_status(self) -> Dict[str, Any]:
        """Retorna status da integra√ß√£o"""
        
        status = {
            "available": self.available,
            "model": self.model,
            "base_url": self.base_url
        }
        
        if self.available:
            try:
                # Testa modelo espec√≠fico
                response = requests.post(
                    f"{self.base_url}/api/generate",
                    json={
                        "model": self.model,
                        "prompt": "Test",
                        "stream": False,
                        "options": {"max_tokens": 5}
                    },
                    timeout=5
                )
                status["model_ready"] = response.status_code == 200
            except:
                status["model_ready"] = False
        
        return status


def create_ollama_integration(model: str = "phi3:mini") -> OllamaIntegration:
    """Factory function para criar integra√ß√£o Ollama"""
    return OllamaIntegration(model=model)